\chapter{A Multi-threaded Web Crawler}

\section{Objectives}
The purpose of this lab is to design and implement a multi-threaded web crawler. In the previous lab, we practiced memory sharing as a means to communicate between processes. Sharing memory between threads is a lot easier since they live in the same address space. We do not need the operating system's involvement to have a shared memory region between threads. In addition, creating/destroying threads is less expensive than creating/terminating child processes. 

However we still need to avoid race conditions in the memory region that threads are sharing. Aside from mutex and semaphore, the operating system also provides condition variable and atomic type facilities. 

After this lab, you will be able to
\begin{itemize}
\item design and implement a multi-threaded concurrent program that requires more than one synchronization pattern
\item gain more experience in the Linux mutex, semaphore, condition variable and atomic type facilities to synchronize threads
\end{itemize}

\section{Starter Files}
The starter files are on GitHub, in the lab4/starter directory. That directory contains the following sub-directories where we have example code to help you get started:

\begin{itemize}
%\item \verb+cURL\_header+ has example code to show how to use curl header call back function to display all the http response headers
%\item \verb+parser+ has example code for parsing an html file and extracting http links
\item \verb+curl_xml+ has example code to show how to use curl and libxml2 together to identify a possible png page and extract http and https links from an html page
\item \verb+tools+ has a shell script to compute statistics from timing data.
\end{itemize}
The file lab4\_eceubunt1.csv is the template that you will need for submitting timing results.

\section{Preparation}
Read the entire lab4 manual to understand what the lab assignment is about. Build and run the starter code to see what it does. You should work through the provided starter code to understand how it works. The following activities will help you to understand the code.
\begin{enumerate}
\item Run the given starter code with the following URLs and examine the responses from the server in the http header. 
  \begin{itemize}
  \item \url{http://ece252-1.uwaterloo.ca/lab4}
  \item \url{http://ece252-1.uwaterloo.ca/lab3/index.html}
  \item \url{http://ece252-1.uwaterloo.ca/~yqhuang/lab4/Disguise.png}
  \item \url{http://ece252-1.uwaterloo.ca:2530/image?img=1&part=1}
  \end{itemize}
\item Execute \code{man pthread\_cond} to read the man page on condition variables.
\end{enumerate}
Linux man pages are also available online at \url{https://linux.die.net/}.

Create a single-threaded implementation of the \verb+findpng2+ command (see Section \ref{sec:findpng2_man}).

\section{Lab Assignment}
\subsection{Problem Statement}
In the previous labs, the URLs to download the image segments were provided. In this lab, you will need to search some HTTP lab servers to find these URLs. We have 50 different URLs, each of which links to a unique PNG image segment of a particular image. The mission is to search for these URLs on the lab servers\footnote{This lab does not require you to concatenate these segments. However, if you are interested in what these segments are, then you can use your catpng to restore the original image after downloading all the segments or directly concatenate the segments in memory using lab2/3 code. The simple PNG format, dimensions of each image segment and the http header that tells you which segment you are getting are the same as what we had in previous labs.}.
%and find all of them and put them in a flat file named \verb+png_urls.txt+, where each line is a PNG image segment URL.

To solve the problem, we will create a multi-threaded web crawler named \verb+findpng2+ to search the web starting from a given seed URL and find all the URLs that link to PNG images.

\subsection{The findpng2 command}
The expected behaviour of the \verb+findpng2+ command is given in the following manual page.
\subsubsection{Man page of findpng2}
\label{sec:findpng2_man}
\subsubsection*{NAME}
\begin{itemize}
	\item[]{\bf findpng2} - search for PNG file URLs on the web
\end{itemize}
\subsubsection*{SYNOPSIS}
\begin{itemize}
	\item[]{\bf findpng2} [OPTION]... SEED\_URL
\end{itemize}
\subsubsection*{DESCRIPTION}
\begin{itemize}
\item[]Start from the SEED\_URL and search for PNG file URLs on the world wide web and store the search results in a plain text file named \code{png\_urls.txt} in the current working directory. Output the execution time in seconds to the standard output.
\item[] {\bf -t}=NUM
  \begin{itemize}
  \item[] create NUM threads simultaneously crawling the web. Each thread uses the curl blocking I/O function to download the data and then processes the downloaded data. The total number of pthread\_create() invocations should be equal to NUM specified by the -t option. When this option is not specified, assumes a single-threaded implementation.
  \end{itemize}
\item[] {\bf -m}=NUM
  \begin{itemize}
  \item[] find up to NUM unique PNG URLs on the web. It is possible that the number of search results is less than NUM. When this option is not specified, assumes NUM=50. 
  \end{itemize}
\item[] {\bf -v}=LOGFILE
  \begin{itemize}
  \item[] log all the URLs visited by the crawler, one URL per line in LOGFILE. When this option is not specified, do not log any visited URLs by the crawler and do not create any visited URLs log file.
  \end{itemize}
\end{itemize}
\subsubsection*{OUTPUT FORMAT}
\begin{itemize}
\item[]The time to execute the program is output to the standard output. It will look like the following:
\begin{verbatim}
findpng2 execution time: S seconds
\end{verbatim}
  The search result is a list of PNG URLs, one URL per line saved in a file named \code{png\_urls.txt}. The order of listing the search results is not specified. If the search result is empty, then create an empty search result file.
\end{itemize}
\subsubsection*{EXAMPLES}
\begin{verbatim}
    findpng2 -t 10 -m 20 -v log.txt http://ece252-1.uwaterloo.ca/lab4
\end{verbatim}
\begin{itemize}
\item[]Find up to 20 PNG URLs starting from \url{http://ece252-1.uwaterloo.ca/lab4} using 10 threads.
The output on the standard output will look like the following:
\begin{verbatim}
findpng2 execution time: 10.123456 seconds
\end{verbatim}
The first two lines in the \code{png\_urls.txt} file may look like the following:
\begin{verbatim}
http://ece252-2.uwaterloo.ca:2540/img?q=tyfoighidfyseoid==
http://ece252-1.uwaterloo.ca:2541/img?q=kjvjkjxsroutqpqkgh
\end{verbatim}
An empty search result will generate \code{an empty png\_urls.txt} file.

The first two lines in the \code{log.txt} file may look like the following:
\begin{verbatim}
http://ece252-1.uwaterloo.ca/lab4
http://ece252-1.uwaterloo.ca/~yqhuang/lab4/index.html
\end{verbatim}
\end{itemize}

\subsection{Web crawling}
The \verb+findpng2+ command is a tiny simplified web crawler. It searches the web by starting from a seed URL. The crawler visits the given URL page and finds two pieces of information.

The first piece is the URLs that link to valid PNG images\footnote{A valid PNG image is a file whose first 8 bytes match the PNG signature bytes}. The crawler adds PNG URLs found to a search result table. We want this table to contain unique URLs, hence if the found URL is already in the table, you should not add it to the table again.

The second piece is a set of new URLs to further crawl. The crawler adds this set of new URLs to a URLs pool known as the \code{URLs frontier}. Since visiting web pages has costs, we do not want the crawler to visit the same page twice. Hence the crawler needs a mechanism to remember URLs that have been visited already. As the crawler visits the URLs in the URLs frontier, the process of finding the target PNG URLs and new URLs to further explore repeats until it finds no more new PNG URLs or it reaches the maximum number of PNG URLs specified on the command line.

\subsection{HTTP}
HTTP stands for ``Hypertext Transfer Protocol''. It carries important information about the client requests and the server responses. When the client sends an URL to the server, it makes an HTTP GET request and the detailed information about the request is in the headers. The server will first respond with an HTTP response status code line. There are three categories we need to handle in this lab.
\begin{itemize}
\item HTTP/1.1 2XX. This is a success response. We need to process the data the link gives.
\item HTTP/1.1 3XX. This is the case if the link has been relocated. By feeding the \code{curl\_easy\_setopt} function the \code{CURLOPT\_FOLLOWLOCATION} option, curl will automatically follow the relocated links.  The \code{CURLOPT\_MAXREDIRS} in the curl option setting allows you to specify the maximum number of redirects to follow.
\item HTTP/1.1 4XX. This is a broken link, usually caused by the client side. We do not process the link, but we need to remember that this link has been visited.
\item HTTP/1.1 5XX. This is also a broken link, usually caused by a server internal error. We are not able to process the link. But we again need to remember this link has been visited.
\end{itemize}

After the response status code line, the web server uses http response headers to send meta information in different fields about the web resource content it sends to the client. One of the fields is ``Content-Type''. For the purpose of this lab, we are only interested in two varieties of Content-Type. One is \code{text/html}, which is an HTML file where we might find more URLs. The other is \code{image/png}, which is a PNG image. You will process both of those content types, and ignore any others you might encounter.

The http header callback function of curl allows us to process all the header responses from the server.
Another way is to use the \code{curl\_easy\_getinfo} function to obtain a specific header\footnote{Only standard headers are supported. User defined headers such as those starting with X- are not supported.}. For example, with the second parameter of that function set to \code{CURLINFO\_CONTENT\_TYPE}, we can obtain the content type header information.

As you may recall from previous labs, the lab server uses an HTTP response header that has the format of ``X-Ece252-Fragment: $M$'' where $M \in [0, 49]$ to indicate which image segment it's sending to the client. If you are only interested in finding the PNG image segments that the lab web server has, then this piece of information is useful.

After all the response headers are sent, the server sends the actual contents of the web resource in the message body. The write callback function of curl allows us to process this piece of information. 
\subsection{Programming Tips}
You will need a number of lists to keep track of different sets of URLs. One list is for the URLs frontier, which contains to-be-visited URLs. One list is for recording all the URLs that have been visited. Another list is for the PNG URLs that have been found. To crawl the web using multiple threads, these lists are shared between threads. Hence you need to synchronize them. Some lists can only be accessed by one thread both when reading and writing. Some lists may be accessed by multiple threads when reading, but only one thread when writing.

Another subtle difficulty is to know when to terminate the program. The program should terminate either when there are no more URLs in the URLs frontier or the user specified number of PNG URLs have been found. You may need some shared counters to keep track of information such as how many PNG URLs have been found and how many threads are waiting for a new URL.

If an URL has been visited already, then we do not want to visit it again. So a list of visited URLs is needed. Hashing will make the search very efficient and you might consider using a hash table to represent this already-visited list. The glibc library has a hash table API (\code{man hsearch(3)}).

\section{Deliverables}
\label{sec:lab4:lab}
Put the following items under a directory named Lab4:
%Submit a compressed archive file that contains the following:
\begin{enumerate}
\item All the source code and a Makefile. The Makefile default target is the \verb+findpng2+ executable file. That is, the command \verb+make+ should generate the \verb+findpng2+ executable file. We also expect that the command \verb+make clean+ will remove the object code and the default target. That is, the \verb+.o+ files and the executable file should be removed.
\item A timing result .csv file named lab4\_hostname.csv 
  which contains the timing results by running the \verb+findpng2+ command on a server whose name is hostname. For example, \verb+lab4_eceubuntu1.csv+ means findpng2 was executed on the server eceubuntu1 and the file contains the timing results.
  The first line of the file is the header of the timing result table. The rest of the rows are the command line argument values and the timing results. The columns of the .csv file from left to right are the values of $T$ (the number of threads), $M$ (the number of unique PNG links to search for) and $TIME$ (the corresponding findpng2 average execution time). We have an example .csv file in the starter code folder named \verb+lab4_eceubuntu1.csv+ for illustration purposes.

Run your \verb+findpng2+ command on eceubuntu1. Record the average timing data for the $(T, M)$ values shown in Table \ref{tb_timing_lab4} for a particular host. Note that for each given $(T, M)$ value in the table, you need to run the program $n$ times and compute the average time. We recommend $n=5$.
\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
T     & M    & Time \\ \hline

1 &    1 &    \\ \hline
1 &    10 &    \\ \hline
1 &    20 &    \\ \hline
1 &    30 &    \\ \hline
1 &    40 &    \\ \hline
1 &    50 &    \\ \hline
1 &    100 &    \\ \hline
10 &    1 &    \\ \hline
10 &    10 &    \\ \hline
10 &    20 &    \\ \hline
10 &    30 &    \\ \hline
10 &    40 &    \\ \hline
10 &    50 &    \\ \hline
10 &    100 &    \\ \hline
20 &    1 &    \\ \hline
20 &    10 &    \\ \hline
20 &    20 &    \\ \hline
20 &    30 &    \\ \hline
20 &    40 &    \\ \hline
20 &    50 &    \\ \hline
20 &    100 &    \\ \hline

\end{tabular}
\caption{Timing measurement data table for given $(T, M)$ values.}
\label{tb_timing_lab4}
\end{center}
\end{table}
\end{enumerate}
Use the \verb+zip+ command to archive and compress the contents of the Lab4 directory and name it \verb+lab4.zip+. We expect that the command \verb+unzip lab4.zip+ will produce a \verb+Lab4+ sub-directory in the current working directory and under the \verb+Lab4+ sub-directory we will find your source code, the Makefile and the lab4\_hostname.csv file.

\section{Marking Rubric}

As with the earlier labs, you should start with a single-threaded implementation. This will let you get the basics working, without the added complexity (and increased performance) of using multiple threads.
You will receive partial marks for this portion.

Table \ref{tb_lab4_rubric} shows the rubric for marking the lab.

\begin{table}[ht]
\begin{center}
\begin{tabular}{|p{2cm}|p{12cm}|}
\hline
Points &Description  \\ \hline
5  & \verb+Makefile+ correctly builds and cleans \\ \hline
20 & Implementation of single-threaded \verb+findpng2+ \\ \hline
75    & Implementation of multi-threaded \verb+findpng2+ \\ \hline
\end{tabular}
\caption{Lab4 Marking Rubric}
\label{tb_lab4_rubric}
\end{center}
\end{table}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main_book"
%%% End:
